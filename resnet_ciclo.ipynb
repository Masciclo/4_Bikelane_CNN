{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenamiento de ResNet18 para clasificar carriles bici\n",
    "\n",
    "## **1. Configuración inicial**\n",
    "\n",
    "### Instalar librerías necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install -c conda-forge opencv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision pandas scikit-learn matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Preprocesamiento de datos**\n",
    "\n",
    "### Cargar la base de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   number_of_bikelane              condition_name  \\\n",
      "0                 104  ci_o_cr_0_tipci_CA_op_ci_0   \n",
      "1                 104  ci_o_cr_0_tipci_CA_op_ci_0   \n",
      "2                 104  ci_o_cr_0_tipci_CA_op_ci_0   \n",
      "3                 104  ci_o_cr_0_tipci_CA_op_ci_0   \n",
      "4                 104  ci_o_cr_0_tipci_CA_op_ci_0   \n",
      "\n",
      "                                individual_file_path  \n",
      "0  D:\\Results\\104\\ci_o_cr_0_tipci_CA_op_ci_0\\date...  \n",
      "1  D:\\Results\\104\\ci_o_cr_0_tipci_CA_op_ci_0\\date...  \n",
      "2  D:\\Results\\104\\ci_o_cr_0_tipci_CA_op_ci_0\\date...  \n",
      "3  D:\\Results\\104\\ci_o_cr_0_tipci_CA_op_ci_0\\date...  \n",
      "4  D:\\Results\\104\\ci_o_cr_0_tipci_CA_op_ci_0\\date...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Cargar la base de datos\n",
    "df = pd.read_csv('D:\\Results\\ciclo_png_directory.csv')\n",
    "\n",
    "# Verificar datos\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dividir en entrenamiento y validación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenamiento: 48001, Validación: 12001\n"
     ]
    }
   ],
   "source": [
    "# Dividir los datos\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, stratify=df['condition_name'], random_state=42)\n",
    "\n",
    "print(f\"Entrenamiento: {len(train_df)}, Validación: {len(val_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aumentar clases minoritarias y subsamplear clases mayoritarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nuevo tamaño de entrenamiento: 43555\n"
     ]
    }
   ],
   "source": [
    "# Aumentar clases 0 y 4\n",
    "augmented_0 = train_df[train_df['condition_name'] == 'ci_o_cr_0_tipci_BAND_op_ci_0'].sample(n=1000, replace=True, random_state=42)\n",
    "augmented_4 = train_df[train_df['condition_name'] == 'ci_o_cr_1_tipci_BAND_op_ci_0'].sample(n=1000, replace=True, random_state=42)\n",
    "\n",
    "# Subsamplear la clase 11\n",
    "subsampled_11 = train_df[train_df['condition_name'] == 'ci_o_cr_1_tipci_VD_op_ci_1'].sample(n=5000, random_state=42)\n",
    "\n",
    "# Crear un nuevo conjunto de entrenamiento equilibrado\n",
    "train_df = pd.concat([\n",
    "    train_df[~train_df['condition_name'].isin(['ci_o_cr_0_tipci_BAND_op_ci_0', 'ci_o_cr_1_tipci_BAND_op_ci_0', 'ci_o_cr_1_tipci_VD_op_ci_1'])],\n",
    "    augmented_0, augmented_4, subsampled_11\n",
    "])\n",
    "\n",
    "print(f\"Nuevo tamaño de entrenamiento: {len(train_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. Preparar datos para PyTorch**\n",
    "\n",
    "### Transformaciones y DataLoader\n",
    "\n",
    "\n",
    "\n",
    "**Resize((224, int(224 * (16 / 9))))**\n",
    "\n",
    "Ajusta el alto a 224 píxeles y calcula el ancho proporcional manteniendo la relación de aspecto 16:9. El nuevo ancho será aproximadamente 398 píxeles.\n",
    "\n",
    "**Pad((0, 0, (224 - width) // 2, 0))**\n",
    "\n",
    "Calcula el padding necesario en los lados izquierdo y derecho para llevar el ancho resultante (398) a 224 píxeles. Solo rellena en los lados horizontales.\n",
    "\n",
    "**Resultado:**\n",
    "\n",
    "Este enfoque se asegura de que:\n",
    "- No haya distorsión en la imagen\n",
    "- Se mantenga el área importante central de la imagen\n",
    "- Las dimensiones sean las requeridas por el modelo (224x224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.transforms import Compose, Normalize, ToTensor, ColorJitter, RandomHorizontalFlip\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Transformaciones\n",
    "def resize_and_pad(image, target_height=224, target_width=224):\n",
    "    \"\"\"Resize the image to maintain aspect ratio, then pad to target dimensions.\"\"\"\n",
    "    original_height, original_width, _ = image.shape\n",
    "    aspect_ratio = original_width / original_height\n",
    "    \n",
    "    # Calculate new dimensions\n",
    "    if aspect_ratio > 1:  # Wider than tall\n",
    "        new_width = target_width\n",
    "        new_height = int(target_width / aspect_ratio)\n",
    "    else:  # Taller than wide\n",
    "        new_height = target_height\n",
    "        new_width = int(target_height * aspect_ratio)\n",
    "    \n",
    "    # Resize image\n",
    "    resized_image = cv2.resize(image, (new_width, new_height), interpolation=cv2.INTER_LINEAR)\n",
    "    \n",
    "    # Calculate padding\n",
    "    pad_top = (target_height - new_height) // 2\n",
    "    pad_bottom = target_height - new_height - pad_top\n",
    "    pad_left = (target_width - new_width) // 2\n",
    "    pad_right = target_width - new_width - pad_left\n",
    "    \n",
    "    # Pad image\n",
    "    padded_image = cv2.copyMakeBorder(\n",
    "        resized_image, pad_top, pad_bottom, pad_left, pad_right, cv2.BORDER_CONSTANT, value=[0, 0, 0]\n",
    "    )\n",
    "    return padded_image\n",
    "\n",
    "# Define transformations\n",
    "class Transformations:\n",
    "    def __init__(self, train=True):\n",
    "        self.train = train\n",
    "        self.color_jitter = ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1)\n",
    "        self.normalize = Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        self.to_tensor = ToTensor()\n",
    "        self.horizontal_flip = RandomHorizontalFlip()\n",
    "    \n",
    "    def __call__(self, image):\n",
    "        # Resize and pad\n",
    "        image = resize_and_pad(image, target_height=224, target_width=224)\n",
    "        # Convert to float32 and normalize to [0, 1]\n",
    "        image = image.astype(np.float32) / 255.0\n",
    "        # Apply color jitter and flip for training\n",
    "        if self.train:\n",
    "            image = self.color_jitter(torch.from_numpy(image.transpose(2, 0, 1)))  # Apply jitter\n",
    "            if torch.rand(1).item() < 0.5:  # Random horizontal flip\n",
    "                image = torch.flip(image, dims=[2])  # Flip along the width dimension\n",
    "        else:\n",
    "            image = torch.from_numpy(image.transpose(2, 0, 1))  # No augmentation for validation\n",
    "        \n",
    "        # Normalize\n",
    "        image = self.normalize(image)\n",
    "        return image\n",
    "\n",
    "train_transforms = Transformations(train=True)\n",
    "val_transforms = Transformations(train=False)\n",
    "\n",
    "# Dataset personalizado\n",
    "class BikeLaneDataset(Dataset):\n",
    "    def __init__(self, dataframe, transforms):\n",
    "        self.dataframe = dataframe\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataframe.iloc[idx]\n",
    "        # Load image with OpenCV\n",
    "        image = cv2.imread(row['individual_file_path'])\n",
    "        if image is None:\n",
    "            raise ValueError(f\"Image not found at {row['individual_file_path']}\")\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB\n",
    "        label = row['condition_name']\n",
    "        label_idx = condition_to_idx[label]  # Convertir etiqueta a índice\n",
    "        if self.transforms:\n",
    "            image = self.transforms(image)\n",
    "        return image, label_idx\n",
    "\n",
    "# Crear DataLoader\n",
    "condition_to_idx = {cond: idx for idx, cond in enumerate(train_df['condition_name'].unique())}\n",
    "\n",
    "train_dataset = BikeLaneDataset(train_df, train_transforms)\n",
    "val_dataset = BikeLaneDataset(val_df, val_transforms)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True, num_workers=6)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=256, shuffle=False, num_workers=6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **4. Entrenar ResNet18**\n",
    "\n",
    "### Definir el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision.models import resnet18\n",
    "\n",
    "# Cargar modelo preentrenado\n",
    "model = resnet18(pretrained=True)\n",
    "\n",
    "# Reemplazar la última capa\n",
    "num_classes = len(condition_to_idx)\n",
    "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "\n",
    "# Usar GPU si está disponible\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.5.1\n",
      "¿GPU disponible?: True\n",
      "Nombre de la GPU: NVIDIA GeForce RTX 4060\n"
     ]
    }
   ],
   "source": [
    "#Verifica que se este usando el CUDA\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"¿GPU disponible?: {torch.cuda.is_available()}\")\n",
    "print(f\"Nombre de la GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'No hay GPU disponible'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definir loss y optimizador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ciclo de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Train Loss: 1900.2883, Train Accuracy: 0.4879\n",
      "Val Loss: 457.2839, Val Accuracy: 0.5486\n",
      "Epoch 2/10\n",
      "Train Loss: 1356.7168, Train Accuracy: 0.6264\n",
      "Val Loss: 359.0026, Val Accuracy: 0.6307\n",
      "Epoch 3/10\n",
      "Train Loss: 1128.6612, Train Accuracy: 0.6844\n",
      "Val Loss: 313.8178, Val Accuracy: 0.6804\n",
      "Epoch 4/10\n",
      "Train Loss: 962.0775, Train Accuracy: 0.7209\n",
      "Val Loss: 297.3737, Val Accuracy: 0.7013\n",
      "Epoch 5/10\n",
      "Train Loss: 851.9297, Train Accuracy: 0.7488\n",
      "Val Loss: 279.7789, Val Accuracy: 0.7165\n",
      "Epoch 6/10\n",
      "Train Loss: 778.2708, Train Accuracy: 0.7654\n",
      "Val Loss: 267.2904, Val Accuracy: 0.7236\n",
      "Epoch 7/10\n",
      "Train Loss: 705.3919, Train Accuracy: 0.7823\n",
      "Val Loss: 257.6091, Val Accuracy: 0.7284\n",
      "Epoch 8/10\n",
      "Train Loss: 659.4561, Train Accuracy: 0.7904\n",
      "Val Loss: 262.8404, Val Accuracy: 0.7151\n",
      "Epoch 9/10\n",
      "Train Loss: 615.3516, Train Accuracy: 0.8042\n",
      "Val Loss: 253.8431, Val Accuracy: 0.7419\n",
      "Epoch 10/10\n",
      "Train Loss: 582.6085, Train Accuracy: 0.8068\n",
      "Val Loss: 265.2738, Val Accuracy: 0.7095\n"
     ]
    }
   ],
   "source": [
    "def train(model, criterion, optimizer, train_loader, val_loader, num_epochs=10):\n",
    "    for epoch in range(num_epochs):\n",
    "        # Entrenamiento\n",
    "        model.train()\n",
    "        train_loss, train_correct = 0, 0\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            train_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "        # Validación\n",
    "        model.eval()\n",
    "        val_loss, val_correct = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "                val_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "        # Métricas\n",
    "        train_accuracy = train_correct / len(train_loader.dataset)\n",
    "        val_accuracy = val_correct / len(val_loader.dataset)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        print(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}\")\n",
    "        print(f\"Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "train(model, criterion, optimizer, train_loader, val_loader, num_epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **5. Evaluar métricas**\n",
    "\n",
    "### Predicciones y cálculo de métricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              precision    recall  f1-score   support\n",
      "\n",
      "  ci_o_cr_1_tipci_VD_op_ci_0       0.65      0.86      0.74      1359\n",
      " ci_o_cr_1_tipci_PAR_op_ci_1       0.78      0.92      0.84       843\n",
      "  ci_o_cr_1_tipci_CA_op_ci_0       0.79      0.86      0.82      1875\n",
      "  ci_o_cr_1_tipci_CA_op_ci_1       0.85      0.77      0.81      1762\n",
      "  ci_o_cr_0_tipci_VD_op_ci_0       0.36      0.48      0.42       994\n",
      "ci_o_cr_1_tipci_BAND_op_ci_1       0.81      0.79      0.80       422\n",
      "  ci_o_cr_0_tipci_CA_op_ci_0       0.40      0.43      0.42       849\n",
      " ci_o_cr_1_tipci_PAR_op_ci_0       0.86      0.91      0.88       791\n",
      " ci_o_cr_0_tipci_PAR_op_ci_0       0.31      0.09      0.14       244\n",
      "ci_o_cr_0_tipci_BAND_op_ci_0       0.41      0.52      0.46       108\n",
      "ci_o_cr_1_tipci_BAND_op_ci_0       0.67      0.87      0.76        38\n",
      "  ci_o_cr_1_tipci_VD_op_ci_1       0.86      0.59      0.70      2716\n",
      "\n",
      "                    accuracy                           0.71     12001\n",
      "                   macro avg       0.65      0.67      0.65     12001\n",
      "                weighted avg       0.73      0.71      0.71     12001\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Obtener predicciones\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            preds = outputs.argmax(1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    return all_preds, all_labels\n",
    "\n",
    "# Predicciones\n",
    "val_preds, val_labels = evaluate(model, val_loader)\n",
    "\n",
    "# Reporte de clasificación\n",
    "print(classification_report(val_labels, val_preds, target_names=condition_to_idx.keys()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Évaluación de una métrica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 3 predictions:\n",
      "Rank 1:\n",
      "Class: ci_o_cr_1_tipci_VD_op_ci_0\n",
      "Label: 0\n",
      "Confidence: 0.8199\n",
      "Rank 2:\n",
      "Class: ci_o_cr_0_tipci_VD_op_ci_0\n",
      "Label: 4\n",
      "Confidence: 0.0653\n",
      "Rank 3:\n",
      "Class: ci_o_cr_1_tipci_VD_op_ci_1\n",
      "Label: 11\n",
      "Confidence: 0.0549\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define the transformation pipeline (same as used during training)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Function to load and preprocess the image\n",
    "def load_image(image_path):\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image = transform(image)\n",
    "    image = image.unsqueeze(0)  # Add batch dimension\n",
    "    return image\n",
    "\n",
    "# Function to evaluate a single image\n",
    "def evaluate_single_image(model, image_path):\n",
    "    model.eval()\n",
    "    image = load_image(image_path).to(device)\n",
    "    with torch.no_grad():\n",
    "        output = model(image)\n",
    "        probabilities = F.softmax(output, dim=1)\n",
    "        top_probs, top_labels = torch.topk(probabilities, 3)\n",
    "        top_probs = top_probs.cpu().numpy().flatten()\n",
    "        top_labels = top_labels.cpu().numpy().flatten()\n",
    "    return top_labels, top_probs\n",
    "\n",
    "# Example usage\n",
    "image_path = r\"D:\\Results\\97\\ci_o_cr_1_tipci_CA_op_ci_0\\date_0_OE_1frame_328.png\"  # Use raw string to avoid escape characters\n",
    "# Alternatively, you can use forward slashes\n",
    "# image_path = 'C:/Users/User/Downloads/imagen_prueba.png'\n",
    "\n",
    "top_labels, top_probs = evaluate_single_image(model, image_path)\n",
    "\n",
    "# Map the predicted labels to the corresponding class names\n",
    "idx_to_condition = {v: k for k, v in condition_to_idx.items()}\n",
    "top_classes = [idx_to_condition[label] for label in top_labels]\n",
    "\n",
    "print(\"Top 3 predictions:\")\n",
    "for i in range(3):\n",
    "    print(f\"Rank {i+1}:\")\n",
    "    print(f\"Class: {top_classes[i]}\")\n",
    "    print(f\"Label: {top_labels[i]}\")\n",
    "    print(f\"Confidence: {top_probs[i]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reporte con imagenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from fpdf import FPDF\n",
    "from PIL import Image\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, val_loader, device):\n",
    "    model.eval()\n",
    "    all_preds, all_labels, all_probs = [], [], []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            probs = torch.nn.functional.softmax(outputs, dim=1)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "    return all_preds, all_labels, all_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(all_labels, all_preds, class_names, title):\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"confusion_matrix.png\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PDF(FPDF):\n",
    "    def header(self):\n",
    "        self.set_font(\"Arial\", \"B\", 12)\n",
    "        self.cell(0, 10, \"Model Evaluation Report\", 0, 1, \"C\")\n",
    "\n",
    "    def footer(self):\n",
    "        self.set_y(-15)\n",
    "        self.set_font(\"Arial\", \"I\", 8)\n",
    "        self.cell(0, 10, f\"Page {self.page_no()}\", 0, 0, \"C\")\n",
    "\n",
    "def create_pdf_report(class_names, true_positives, false_positives, metrics, overall_metrics):\n",
    "    pdf = PDF()\n",
    "    pdf.add_page()\n",
    "\n",
    "    # Add overall metrics\n",
    "    pdf.set_font(\"Arial\", \"B\", 12)\n",
    "    pdf.cell(0, 10, \"Overall Metrics\", 0, 1)\n",
    "    pdf.set_font(\"Arial\", \"\", 10)\n",
    "    for metric, value in overall_metrics.items():\n",
    "        pdf.cell(0, 10, f\"{metric}: {value:.4f}\", 0, 1)\n",
    "\n",
    "    # Add confusion matrix\n",
    "    pdf.add_page()\n",
    "    pdf.set_font(\"Arial\", \"B\", 12)\n",
    "    pdf.cell(0, 10, \"Confusion Matrix\", 0, 1)\n",
    "    pdf.image(\"confusion_matrix.png\", x=10, y=20, w=180)\n",
    "\n",
    "    # Add class-specific metrics and examples\n",
    "    for class_name in class_names:\n",
    "        pdf.add_page()\n",
    "        pdf.set_font(\"Arial\", \"B\", 12)\n",
    "        pdf.cell(0, 10, f\"Class: {class_name}\", 0, 1)\n",
    "\n",
    "        # Add metrics\n",
    "        pdf.set_font(\"Arial\", \"\", 10)\n",
    "        for metric, value in metrics[class_name].items():\n",
    "            pdf.cell(0, 10, f\"{metric}: {value:.4f}\", 0, 1)\n",
    "\n",
    "        # Add confidence distribution plot\n",
    "        pdf.image(f\"confidence_distribution_{class_name}.png\", x=10, y=50, w=180)\n",
    "\n",
    "        # Add true positives\n",
    "        pdf.add_page()\n",
    "        pdf.set_font(\"Arial\", \"B\", 12)\n",
    "        pdf.cell(0, 10, \"True Positives\", 0, 1)\n",
    "        for example in true_positives[class_name]:\n",
    "            pdf.set_font(\"Arial\", \"\", 10)\n",
    "            pdf.cell(0, 10, f\"Image: {example['image_path']}\", 0, 1)\n",
    "            pdf.cell(0, 10, f\"Ground Truth: {example['true_label']}\", 0, 1)\n",
    "            pdf.cell(0, 10, f\"Predicted: {example['predicted_label']}\", 0, 1)\n",
    "            pdf.cell(0, 10, f\"Confidence: {example['confidence']:.4f}\", 0, 1)\n",
    "            pdf.image(example['image_path'], x=10, y=pdf.get_y(), w=50)\n",
    "            pdf.ln(60)\n",
    "\n",
    "        # Add false positives\n",
    "        pdf.add_page()\n",
    "        pdf.set_font(\"Arial\", \"B\", 12)\n",
    "        pdf.cell(0, 10, \"False Positives\", 0, 1)\n",
    "        for example in false_positives[class_name]:\n",
    "            pdf.set_font(\"Arial\", \"\", 10)\n",
    "            pdf.cell(0, 10, f\"Image: {example['image_path']}\", 0, 1)\n",
    "            pdf.cell(0, 10, f\"Ground Truth: {example['true_label']}\", 0, 1)\n",
    "            pdf.cell(0, 10, f\"Predicted: {example['predicted_label']}\", 0, 1)\n",
    "            pdf.cell(0, 10, f\"Confidence: {example['confidence']:.4f}\", 0, 1)\n",
    "            pdf.image(example['image_path'], x=10, y=pdf.get_y(), w=50)\n",
    "            pdf.ln(60)\n",
    "\n",
    "    # Save the PDF\n",
    "    pdf.output(\"D:\\Results\\model_evaluation_report.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plot_confidence_distribution' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 44\u001b[0m\n\u001b[0;32m     42\u001b[0m     tp_confidences \u001b[38;5;241m=\u001b[39m [example[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconfidence\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m example \u001b[38;5;129;01min\u001b[39;00m true_positives[class_name]]\n\u001b[0;32m     43\u001b[0m     fp_confidences \u001b[38;5;241m=\u001b[39m [example[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconfidence\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m example \u001b[38;5;129;01min\u001b[39;00m false_positives[class_name]]\n\u001b[1;32m---> 44\u001b[0m     \u001b[43mplot_confidence_distribution\u001b[49m(tp_confidences, fp_confidences, class_name)\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# Create the PDF report\u001b[39;00m\n\u001b[0;32m     47\u001b[0m create_pdf_report(class_names, true_positives, false_positives, overall_metrics, overall_metrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmacro avg\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plot_confidence_distribution' is not defined"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "all_preds, all_labels, all_probs = evaluate_model(model, val_loader, device)\n",
    "\n",
    "# Get class names\n",
    "class_names = list(condition_to_idx.keys())\n",
    "\n",
    "# Calculate overall metrics\n",
    "overall_metrics = classification_report(all_labels, all_preds, target_names=class_names, output_dict=True)\n",
    "\n",
    "# Generate confusion matrix\n",
    "plot_confusion_matrix(all_labels, all_preds, class_names, \"Confusion Matrix\")\n",
    "\n",
    "# Initialize dictionaries to store true positives and false positives\n",
    "true_positives = {class_name: [] for class_name in class_names}\n",
    "false_positives = {class_name: [] for class_name in class_names}\n",
    "\n",
    "# Populate true positives and false positives\n",
    "for idx, (true_label, pred_label, probs) in enumerate(zip(all_labels, all_preds, all_probs)):\n",
    "    true_class = class_names[true_label]\n",
    "    pred_class = class_names[pred_label]\n",
    "    confidence = probs[pred_label]\n",
    "\n",
    "    if true_label == pred_label:\n",
    "        if len(true_positives[true_class]) < 10:\n",
    "            true_positives[true_class].append({\n",
    "                \"image_path\": val_df.iloc[idx]['individual_file_path'],\n",
    "                \"true_label\": true_class,\n",
    "                \"predicted_label\": pred_class,\n",
    "                \"confidence\": confidence\n",
    "            })\n",
    "    else:\n",
    "        if len(false_positives[pred_class]) < 10:\n",
    "            false_positives[pred_class].append({\n",
    "                \"image_path\": val_df.iloc[idx]['individual_file_path'],\n",
    "                \"true_label\": true_class,\n",
    "                \"predicted_label\": pred_class,\n",
    "                \"confidence\": confidence\n",
    "            })\n",
    "\n",
    "# Generate confidence distribution plots\n",
    "for class_name in class_names:\n",
    "    tp_confidences = [example['confidence'] for example in true_positives[class_name]]\n",
    "    fp_confidences = [example['confidence'] for example in false_positives[class_name]]\n",
    "    plot_confidence_distribution(tp_confidences, fp_confidences, class_name)\n",
    "\n",
    "# Create the PDF report\n",
    "create_pdf_report(class_names, true_positives, false_positives, overall_metrics, overall_metrics['macro avg'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **6. guardar el modelo**\n",
    "\n",
    "### Predicciones y cálculo de métricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model weights saved to D:\\Results\\model_weights.pth\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Assuming 'model' is your trained model\n",
    "model_path = 'D:\\Results\\model_weights.pth'\n",
    "torch.save(model.state_dict(), model_path)\n",
    "print(f\"Model weights saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Assuming 'MyModel' is your model class\n",
    "# model = MyModel()\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "print(f\"Model weights loaded from {model_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BIKELANE_CNN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
